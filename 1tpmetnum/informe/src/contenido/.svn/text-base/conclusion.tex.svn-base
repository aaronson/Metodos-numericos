\section{Conclusión}

Este TP consistió de dos partes, una teórica, donde analizamos la propagación de los errores teóricos, y una práctica, donde observamos eso en los programas que escribimos. Primero, obtuvimos una cota para el error que surge de cortar la sumatoria en un término, que fue menor al valor absoluto del término siguiente. Al momento de realizar el cálculo del error de propagación de la serie de McLaurin obtuvimos funciones recursivas de la propagación del error, lo que no nos permitía expresar la misma de manera clara. La solucion fue desarrollar esa función hasta lograr la formula cerrada. Al final pudimos observar la magnitud del error que puede producirse al realizar dicha cuenta.

Al analizar la función de forma empírica, comenzamos calculando el coseno con precisión nativa y términos fijos. Lo que primero fue detectado fue la forma en que iban intercambiándose los resultados de la aplicación desarrollada por nosotros, en que según cada término el resultado se pasaba del valor real ya sea por arriba o por abajo. Comparando los valores con el coseno implementado en la clase math notamos que el error siempre disminuía y que llegaba un punto en que la precisión de la impresión del resultado y de la herramienta que usamos para graficar era menor que la magnitud del error cometido en el cálculo, con lo cual consideramos que para ese punto el cálculo es suficientemente preciso.

Otra de las cosas que pudimos observar tiene que ver en la diferencia que tenia la serie de MacLaurin cuando trabaja con valores cercanos al 0 y cuando lo hace con valores alejados del mismo. Observamos que mientras en valores cercanos al 0 con una cantidad de términos $n$ los resultados que se obtienen son muy precisos, en valores mas alejados los resultados obtenidos son poco acertados aún aumentando en gran cantidad la cantidad de terminos. Esto pudimos observar que comenzaba a ocurrir cuando cuando $x^{2i} > 2i!$, concluimos que esto ocurre porque cada termino es mayor al anterior por lo que a medida que vamos incrementandolos en vez de acercarse al resultado esperado se va alejando, un mayor $n$ causa que este punto de quiebre se alcance más lentamente, si $n = \infty$, ese punto de quiebre no existiría nunca y la función sería exactamente igual a $\cos (x)$, pero como $n$ es finito, la aproximación no es exacta.

Por otro lado, teníamos que aproximar la función coseno de la misma manera pero con un tamaño de mantisa variable para el double, este tamaño debía poder ser pasado como parámetro al programa. Podemos ver que se obtiene una menor precisión si se limita mucho la cantidad de dígitos de la mantisa, porque se propagan errores mayores porque $\beta^{1-t}$ es más grande, al ser $t$ más pequeño, esto causa que cada operación tenga un error aparejado más grande y por ende el resultado final diste más del teórico. Pero a su vez, el limitar la mantisa (en un número razonable) trae aparejado menor error que el limitar la cantidad de términos, esto es algo a tener en cuenta, porque si tuviéramos problemas de performance y quisiéramos sacrificar un poco de exactitud en aras de obtener un cálculo más rápido, podemos observar que acortar el tamaño de la mantisa causa menor error absoluto que quitar términos de la serie.

Por último, hemos notado que los valores que obtuvimos como resultados luego de la ejecución de nuestro programa, 
en muchos casos han logrado errores pequeños entre el resultado de nuestra función y la función a calcular
Sin embargo al realizar los mismos cálculos simulando un programa que representa el análisis teórico, concluimos que 
en la mayor cantidad de corridas realizadas el error obtenido fue mucho más chicos, lo que la hace a esta forma más fiel para obtener mejores resultados.
