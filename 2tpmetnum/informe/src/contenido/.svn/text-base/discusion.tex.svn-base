\section{Discusión}

Como se puede observar en los gráficos 1.1 y 1.2, las curvas que grafican la isoterma de los diferentes puntos tienden a acercarse más y mas, con la de 5 círculos estando más alejada y ya las de 45, 65 y 85 siendo bastante más parecidas. Esto significa que la discretización es mucho más precisa, y que el valor verdadero (el que se obtendría si tuviéramos la función de la temperatura para ese radio) está siendo mejor aproximado mientras mayor cantidad de puntos tenga la discretización. Es decir, que con una cantidad infinita de puntos ésta sería igual a la función misma.

Igual es necesario recalcar que, aún con su imprecisión, una cantidad menor de puntos da igualmente un resultado bastante bien aproximado, ya que, por ejemplo, en el primer gráfico, en ningún caso el valor de la isoterma para 5 círculos se aleja en más de $0.01$, o sea, un error relativo del $1\%$. Para el 2do gráfico observamos un comportamiento similar, el error relativo se mantiene alrededor del $1\%$. Por lo tanto, se puede asumir que, si bien es inexacto, incluso una discretización bastante chica nos aproxima bien el resultado.

En el gŕafico 2.1 podemos ver que introducir un caso patológico que alterne temperaturas exteriores entre 50 y 200 causa una curva que fluctúa constantemente con cambios bruscos, en contraste con la curva que no es patológica que se comporta de manera más estable. Pero es interesante analizar que, a pesar de tener la mitad de las temperaturas exteriores en $50º$, la isoterma sigue quedando más cerca del exterior del horno que la curva no patológica en todos los casos. Esto se debe, podemos inferir, a que al calcular los valores del radio en que la temperatura externa vale $50º$, a los costados la misma vale $200º$, lo cual modifica mucho el valor y esto hace que los $400º$ se encuentren más afuera que lo que sucede con una temperatura exterior más estable.

Por su parte, el gráfico 3.1 exhibe que el tiempo que insume todo el algoritmo es al menos cuadrático con respecto a la cantidad de puntos. Analizando el algoritmo, sabemos que resolver la eliminación gaussiana tiene un costo $O(n^3)$, siendo $n$ la longitud de la matriz (en este caso, es cantidad de círculos x cantidad de ángulos, o sea, la cantidad de puntos totales), lo que significa que nuestro algoritmo completo también es $O(n^3)$. Probablemente si pudiéramos seguir analizando el comportamiento para más y más puntos observaríamos ese comportamiento cúbico, pero el problema es que una matriz tan grande causa que el sistema se quede sin memoria, por lo que no nos es posible proseguir nuestras mediciones mucho más allá de estos datos que obtuvimos.

Se puede notar que esta curva es bastante irregular, esto lo podemos atribuir al hecho que estamos midiendo algo tan inexacto como el tiempo insumido por un proceso en un sistema que corre muchísimos procesos a la vez, por lo tanto, existen muchos factores que no podemos controlar como memoria disponible, quantums asignados a nuestro proceso, etc., que afectan al resultado final.

Todos estos análisis los hicimos teniendo en cuenta que la pared interna del horno es perfectamente circular. Cabe pensar qué haríamos si ésta no lo fuera, es decir, si fuera irregular. Habiendo discutido el problema, decidimos que lo más logico, suponiendo que nos pasen la función que describa la forma de la pared, sería hacernos una función que nos devuelva si un punto, dada su posición en el horno, si está adentro de la pared o no. Con esta función, al construir la matriz, preguntaríamos si el punto está adentro o no a la hora de hacer las ecuaciones, si nos da $true$, directamente le seteamos la ecuación $x_i = 1500$, si no, estimamos el punto como venimos haciendo. Es posible que hubiera que adaptar el método de resolver escalonado para que soporte este cambio (el de eliminación gaussiana no creemos que requiera cambios), pero no creemos que suponga una gran modificación a nuestro programa.